{
  "numStartups": 11,
  "installMethod": "unknown",
  "autoUpdates": true,
  "tipsHistory": {
    "new-user-warmup": 1,
    "memory-command": 2,
    "theme-command": 3,
    "prompt-queue": 4,
    "enter-to-steer-in-relatime": 5,
    "todo-list": 6,
    "# for memory": 7,
    "install-github-app": 8,
    "drag-and-drop-images": 9,
    "double-esc": 10,
    "continue": 11
  },
  "promptQueueUseCount": 21,
  "firstStartTime": "2025-08-03T22:53:28.573Z",
  "userID": "f9160ee3ccde73943704a3a7200bf4ac9c0c83b2490e02436b0c1a4c1d75c34d",
  "projects": {
    "/home/ubuntu": {
      "allowedTools": [],
      "history": [
        {
          "display": "whats the delay for?",
          "pastedContents": {}
        },
        {
          "display": "let's do phase 1, but before that let's commit everything, run a job, make sure it finishes and then push the code and then start the enxt steps",
          "pastedContents": {}
        },
        {
          "display": "what are the other optimizations to think of for model serving? What are the improvements you can think of in the forntend?",
          "pastedContents": {}
        },
        {
          "display": "looks good, but add it for all images",
          "pastedContents": {}
        },
        {
          "display": "i pushed it ",
          "pastedContents": {}
        },
        {
          "display": "go for it",
          "pastedContents": {}
        },
        {
          "display": "i am fine doing it. but test it locally before we opush to docker",
          "pastedContents": {}
        },
        {
          "display": "check now. is it a success?",
          "pastedContents": {}
        },
        {
          "display": "error\":\"The directory '/data' does not exist\"",
          "pastedContents": {}
        },
        {
          "display": "ok  i pushed it",
          "pastedContents": {}
        },
        {
          "display": "i want to build the mochi-backend from it's folder : /home/ubuntu/backed as that is fadter. rewrite docker command for that",
          "pastedContents": {}
        },
        {
          "display": "which folder do i build this from",
          "pastedContents": {}
        },
        {
          "display": "the build failred you idiot. let me build and push. put the commands here ",
          "pastedContents": {}
        },
        {
          "display": "are loading one model on 1 gpu? if so, i want to have 2 replicas, but each ray-worker to use 2 gpus. ",
          "pastedContents": {}
        },
        {
          "display": "can you check if one of the ray workers is taking one a load? i have submitted a job again",
          "pastedContents": {}
        },
        {
          "display": "i started a job, it is coming as penidng",
          "pastedContents": {}
        },
        {
          "display": "check now",
          "pastedContents": {}
        },
        {
          "display": "are you sure? there was always a ray-head and ray workers? can you check ",
          "pastedContents": {}
        },
        {
          "display": "why do we have only 1 ray worker? and why is tehre no frontend?",
          "pastedContents": {}
        },
        {
          "display": "i deleted by mistake --> export KUBECONFIG=/home/ubuntu/guru-kubeconfig.yml && kubectl delete namespace mochi-video-gen\nnamespace \"mochi-video-gen\" deleted. restart your setup again",
          "pastedContents": {}
        },
        {
          "display": "hang on",
          "pastedContents": {}
        },
        {
          "display": "why are we still getting disk pressure after clearing everything?\ncan we just kill kubernetes, clean up everything and then restart?",
          "pastedContents": {}
        },
        {
          "display": "can you quickly restart the pods? with this new config? also validate that this will not ignore text before this. cant you just torch_dtype=torch.float16",
          "pastedContents": {}
        },
        {
          "display": "ok. how do we fix the volume mounting multiple times?",
          "pastedContents": {}
        },
        {
          "display": "are you sure they are pulling the new image? do you see the logs?",
          "pastedContents": {}
        },
        {
          "display": "why is the ray image so huge? did we goof up somewhere?",
          "pastedContents": {}
        },
        {
          "display": "can you quickly check if the container are still coming up for ray workers?",
          "pastedContents": {}
        },
        {
          "display": "well, lets start a job and try?",
          "pastedContents": {}
        },
        {
          "display": "why are we not using the 15 tb volume for everything?",
          "pastedContents": {}
        },
        {
          "display": "look at the container logs for ray worker, anything wrong there?",
          "pastedContents": {}
        },
        {
          "display": "one of the ray head nodes is dead",
          "pastedContents": {}
        },
        {
          "display": "push to git, restart the pods",
          "pastedContents": {}
        },
        {
          "display": "check ray and ray worker and also check main.py before you apply the configmap",
          "pastedContents": {}
        },
        {
          "display": "check the redis service deplpoyment. what is it actually called and exported as? find what the real name should be, confirm it and then rename?",
          "pastedContents": {}
        },
        {
          "display": "you keep flipping this from redis to redis-service. can you make sure you have only redis-service everywhere in all deployment configs and main.py?",
          "pastedContents": {}
        },
        {
          "display": "pushed to docker, push my changes to git and let's run a job",
          "pastedContents": {}
        },
        {
          "display": "give me the commands i will do the build and push",
          "pastedContents": {}
        },
        {
          "display": "your docker build failed",
          "pastedContents": {}
        },
        {
          "display": "does my main.py have a number of frames? if so, can we switch to an input parameter which defaults to 85 frames, but can take up to 1000? also for the disk pressure, why do we need to make the change? can we not just kill all the stuff, empty the disk, move to the 15tb volume and be done",
          "pastedContents": {}
        },
        {
          "display": "can we move all disks to use the volume with 18tb?",
          "pastedContents": {}
        },
        {
          "display": "let's do both",
          "pastedContents": {}
        },
        {
          "display": "let's check by submitting a job",
          "pastedContents": {}
        },
        {
          "display": "i pushed them all, lets test?",
          "pastedContents": {}
        },
        {
          "display": "why are you removing copying of main.py?",
          "pastedContents": {}
        },
        {
          "display": "approach 1 is great. remember to push this to latest once it is all functional",
          "pastedContents": {}
        },
        {
          "display": "would it cause issues in the future? ",
          "pastedContents": {}
        },
        {
          "display": "the folder structure is wrong for ray -->  > [5/6] COPY backend/main.py /app/main.py:\n------\nDockerfile:20\n--------------------\n  18 |     \n  19 |     # Copy the backend files so Ray workers can access main.py\n  20 | >>> COPY backend/main.py /app/main.py\n  21 |     \n  22 |     # Create a startup script that preloads the model when Ray worker starts\n--------------------\nERROR: failed to build: failed to solve: failed to compute cache key: failed to calculate checksum of ref 93fad93c-7daa-49e3-9d12-eab9fd759205::wom5kcc6du8rsqsczg4ha8gf6: \"/backend/main.py\": not found. i am tryinh from inside /home/ubuntu/ray",
          "pastedContents": {}
        },
        {
          "display": "i will do the image building myseklf",
          "pastedContents": {}
        },
        {
          "display": "done",
          "pastedContents": {}
        },
        {
          "display": "gururajkosuru",
          "pastedContents": {}
        },
        {
          "display": "can you push to git now?",
          "pastedContents": {}
        },
        {
          "display": "i added all the ssh to github. can you run commands to create repository and push?",
          "pastedContents": {}
        },
        {
          "display": "how would i create the respository in github? and then push the changes",
          "pastedContents": {}
        },
        {
          "display": "use gururaj.kosuru@gmail.com",
          "pastedContents": {}
        },
        {
          "display": "can you add the entire project to github, make all commits prior to this warm change, then add and commit this change, then fnally test and debug this",
          "pastedContents": {}
        },
        {
          "display": "isnt the ray worker supposed to hold the warm model?",
          "pastedContents": {}
        },
        {
          "display": "Option 1: Pre-load Model in Ray Worker Initialization <-- i want to do this",
          "pastedContents": {}
        },
        {
          "display": "can we keep the model warm and always loaded in ray? ",
          "pastedContents": {}
        },
        {
          "display": "can you add port forwarding so i can access the frontend? what is the command?",
          "pastedContents": {}
        },
        {
          "display": "whats the status of the ray worker now? can you keep probing every 60 seconds?",
          "pastedContents": {}
        },
        {
          "display": "alright. i pushed. can you ensure the latest is pulled or do you want another tag to ensure you have the right image pulled?",
          "pastedContents": {}
        },
        {
          "display": "which folder is this from",
          "pastedContents": {}
        },
        {
          "display": "dont push to docker, share the commands i will do it",
          "pastedContents": {}
        },
        {
          "display": "go for it and do it all in one go rather than ask permission for each line. ",
          "pastedContents": {}
        },
        {
          "display": "i need logs in side the ray worker part. have you added them in main.py within ray worker execution code? also is the gpu = 1 correct? review the ray parts of the code closely and add debug logs before testing the logging.py",
          "pastedContents": {}
        },
        {
          "display": "hang on first of all, can you add logs to main.py so that we can see at every point, when redis is called, when ray worker is called and so on. and can you add exception handling so that we see logs on failures and status in ray worker and main.py? along with test scripts, our main.py needs to have enough loogging and exception handling so we know instead of flying blind",
          "pastedContents": {}
        },
        {
          "display": "can you check the ray workers to make sure someone has taken rthe job and is processing?",
          "pastedContents": {}
        },
        {
          "display": "do ray workers not need redis? why are you removing it?",
          "pastedContents": {}
        },
        {
          "display": "dont hard code, it will cause issues",
          "pastedContents": {}
        },
        {
          "display": "test it for me",
          "pastedContents": {}
        },
        {
          "display": "let's get it done",
          "pastedContents": {}
        },
        {
          "display": "why is the ray head executing tasks? look in main.py and look for what is being done, looks like the ray config is incorrect",
          "pastedContents": {}
        },
        {
          "display": "how do we debug more?",
          "pastedContents": {}
        },
        {
          "display": "check if the job is finished, check if the gpu workers have load on them",
          "pastedContents": {}
        },
        {
          "display": "let's test with a prompt then",
          "pastedContents": {}
        },
        {
          "display": "before you do that, can you check what disk space we have now? has it improved?",
          "pastedContents": {}
        },
        {
          "display": "can you check where the disk pressure is coming from? let's free up the volumes",
          "pastedContents": {}
        },
        {
          "display": "why is one of the backend pods up since 10 hours? dont we need to kill it? ",
          "pastedContents": {}
        },
        {
          "display": "pushed",
          "pastedContents": {}
        },
        {
          "display": "should we also tag and push to latest? share the command",
          "pastedContents": {}
        },
        {
          "display": "i will run the command myself and push. i will let you know when done. you restart the pods",
          "pastedContents": {}
        },
        {
          "display": "stop the build. i want to test using kubernetes, not docker-compose",
          "pastedContents": {}
        },
        {
          "display": "let's start on next steps",
          "pastedContents": {}
        },
        {
          "display": "read WORK_SUMMARY_2025-08-04.md for the status",
          "pastedContents": {}
        },
        {
          "display": "you wer building pushing all dependent images in gubernetes project to docker and resolving ray and redis issues",
          "pastedContents": {}
        },
        {
          "display": "make all the changes for redis and ray and push to docker. you have the docker login success already",
          "pastedContents": {}
        },
        {
          "display": "lets do it",
          "pastedContents": {}
        },
        {
          "display": "jobs are stillin pending",
          "pastedContents": {}
        },
        {
          "display": "lets solve ray worker issue",
          "pastedContents": {}
        },
        {
          "display": "now?",
          "pastedContents": {}
        },
        {
          "display": "[8:42:36 PM]Failed to submit job: 500 - Internal Server Error",
          "pastedContents": {}
        },
        {
          "display": "what is port forwrd ing for? and why do you need to kill it?",
          "pastedContents": {}
        },
        {
          "display": "[8:26:08 PM]Failed to submit job: 500 - Internal Server Error",
          "pastedContents": {}
        },
        {
          "display": "you need to use the guru_kubecontrol.yml or some such similar file in /home/ubuntu",
          "pastedContents": {}
        },
        {
          "display": "now that it is pushed, let's re-run the kubernetes",
          "pastedContents": {}
        },
        {
          "display": "did the last push work? sudo docker push glususer/mochi-backend:latest",
          "pastedContents": {}
        },
        {
          "display": "you were trying to build and push to docker  when you crashed : sudo docker build -t glususer/mochi-backend:latest ./backend",
          "pastedContents": {}
        },
        {
          "display": "[8:04:04 PM]Failed to submit job: 500 - Internal Server Error",
          "pastedContents": {}
        },
        {
          "display": "i am on a remote instance i am trying to port forward via ssh tunnel to my local host .bind [127.0.0.1]:3000: Address already in use\nchannel_setup_fwd_listener_tcpip: cannot listen to port: 3000\nCould not request local forwarding.\n\ndo you know what is wrong",
          "pastedContents": {}
        },
        {
          "display": "can you tell me what port i can view the frontend on ?",
          "pastedContents": {}
        }
      ],
      "mcpContextUris": [],
      "mcpServers": {},
      "enabledMcpjsonServers": [],
      "disabledMcpjsonServers": [],
      "hasTrustDialogAccepted": false,
      "projectOnboardingSeenCount": 0,
      "hasClaudeMdExternalIncludesApproved": false,
      "hasClaudeMdExternalIncludesWarningShown": false,
      "hasCompletedProjectOnboarding": true,
      "lastTotalWebSearchRequests": 0
    },
    "/home/ubuntu/backend": {
      "allowedTools": [],
      "history": [
        {
          "display": " i think all jobs are just stuck in runnning  : f80586dc-860a-41f2-8116-cead1215ff8d",
          "pastedContents": {}
        },
        {
          "display": "i dont see ray-workers",
          "pastedContents": {}
        },
        {
          "display": "[Pasted text #1 +36 lines]",
          "pastedContents": {
            "1": {
              "id": 1,
              "type": "text",
              "content": "[2025-08-04 00:33:55,920 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:34:25,921 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:34:31,926 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:35:01,927 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:35:07,932 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:35:37,933 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:35:43,938 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:36:13,939 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:36:19,943 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:36:49,944 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:36:55,949 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:37:25,950 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:37:31,955 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:38:01,955 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:38:07,960 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:38:37,961 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:38:43,966 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:39:13,967 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:39:19,972 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:39:49,973 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:39:55,978 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:40:25,979 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:40:31,984 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:41:01,985 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:41:07,990 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:41:37,991 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:41:43,996 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:42:13,997 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:42:20,001 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:42:50,002 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:42:56,007 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:43:26,008 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:43:32,013 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:44:02,014 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:44:08,018 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds.\n[2025-08-04 00:44:38,019 W 1 1] gcs_client.cc:183: Failed to get cluster ID from GCS server: TimedOut: Timed out while waiting for GCS to become available.\n[2025-08-04 00:44:44,024 W 1 1] gcs_rpc_client.h:151: Failed to connect to GCS at address 192.168.0.35:6380 within 5 seconds."
            }
          }
        },
        {
          "display": "check now",
          "pastedContents": {}
        },
        {
          "display": "we have 8 H100 GPUs, how much vram do they have? let's go close to that number",
          "pastedContents": {}
        },
        {
          "display": "why not give the ray workers more memory? each gpu has 80 GB i guess, so we should go close to that?",
          "pastedContents": {}
        },
        {
          "display": "[Pasted text #1 +3 lines]",
          "pastedContents": {
            "1": {
              "id": 1,
              "type": "text",
              "content": "(raylet) [2025-08-04 00:24:39,978 E 917 917] (raylet) node_manager.cc:3041: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: e7fb08689f5309da0fca548196f1215eeae7172830caa9ad72f2dac9, IP: 192.168.0.246) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.0.246`\n(raylet) \n(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
            }
          }
        },
        {
          "display": "done",
          "pastedContents": {}
        },
        {
          "display": "lets not create the directory at all. the /data/videos has acccess right. can you confirm? if so, let's remove the creation /data completely?",
          "pastedContents": {}
        },
        {
          "display": "done",
          "pastedContents": {}
        },
        {
          "display": "also push to latest?",
          "pastedContents": {}
        },
        {
          "display": "maybe change main.py to not create if already exists?",
          "pastedContents": {}
        },
        {
          "display": "lets fix the /data issue then",
          "pastedContents": {}
        },
        {
          "display": "i already pushed it ray-client-fix\n\nLast pushed 5 minutes by glususer",
          "pastedContents": {}
        },
        {
          "display": "use the ray client fix tag?",
          "pastedContents": {}
        },
        {
          "display": "pushed",
          "pastedContents": {}
        },
        {
          "display": "ok, write the commands to docker build and tag and push, but let me do it, and do with sudo docker",
          "pastedContents": {}
        },
        {
          "display": "i check if the pods are up",
          "pastedContents": {}
        },
        {
          "display": "pushed, lets try now",
          "pastedContents": {}
        },
        {
          "display": "let's add that to the Dockerfile, then let me know. i will push it. then let's restart",
          "pastedContents": {}
        },
        {
          "display": "why dont you use my guru-kubeconfig to restart so backend pulls latest docker and make a request to test?",
          "pastedContents": {}
        },
        {
          "display": "does the connection to ray and redis from main.py, ray head and ray workers all look good?",
          "pastedContents": {}
        },
        {
          "display": "i dont think that was the problem. ray workers were unable to connect to ray head",
          "pastedContents": {}
        },
        {
          "display": "let's use redis everywhere in kubernetes too. remove any instances of redis-service for consistency",
          "pastedContents": {}
        },
        {
          "display": "can you check what is the redis variable name from redis and other deploymewnt configs?",
          "pastedContents": {}
        },
        {
          "display": "claude code died. it is working on my gubernates folder project where ray is having connectivity issues. it might have tried to hard code ray ip",
          "pastedContents": {}
        },
        {
          "display": "can you investigate the configuration issue? ",
          "pastedContents": {}
        },
        {
          "display": "restart my kubernetes using my guru-kubeconfig.yml file and make sure the newly pushed docker images are pulled",
          "pastedContents": {}
        },
        {
          "display": "whats the command to docker build all my images for project in folder gubernates. i want to use sudo docker build and then push",
          "pastedContents": {}
        }
      ],
      "mcpContextUris": [],
      "mcpServers": {},
      "enabledMcpjsonServers": [],
      "disabledMcpjsonServers": [],
      "hasTrustDialogAccepted": true,
      "projectOnboardingSeenCount": 6,
      "hasClaudeMdExternalIncludesApproved": false,
      "hasClaudeMdExternalIncludesWarningShown": false,
      "lastTotalWebSearchRequests": 0
    }
  },
  "oauthAccount": {
    "accountUuid": "84ea325b-7c48-47ad-a0e1-66bd471caf14",
    "emailAddress": "glususer@gmail.com",
    "organizationUuid": "af573f2e-6c15-47e3-94e3-0f172f0c2983",
    "organizationRole": "admin",
    "workspaceRole": null,
    "organizationName": "glususer@gmail.com's Organization"
  },
  "isQualifiedForDataSharing": false,
  "hasCompletedOnboarding": true,
  "lastOnboardingVersion": "1.0.67",
  "cachedChangelog": "# Changelog\n\n## 1.0.65\n\n- IDE: Fixed connection stability issues and error handling for diagnostics\n- Windows: Fixed shell environment setup for users without .bashrc files\n\n## 1.0.64\n\n- Agents: Added model customization support - you can now specify which model an agent should use\n- Agents: Fixed unintended access to the recursive agent tool\n- Hooks: Added systemMessage field to hook JSON output for displaying warnings and context\n- SDK: Fixed user input tracking across multi-turn conversations\n- Added hidden files to file search and @-mention suggestions\n\n## 1.0.63\n\n- Windows: Fixed file search, @agent mentions, and custom slash commands functionality\n\n## 1.0.62\n\n- Added @-mention support with typeahead for custom agents. @<your-custom-agent> to invoke it\n- Hooks: Added SessionStart hook for new session initialization\n- /add-dir command now supports typeahead for directory paths\n- Improved network connectivity check reliability\n\n## 1.0.61\n\n- Transcript mode (Ctrl+R): Changed Esc to exit transcript mode rather than interrupt\n- Settings: Added `--settings` flag to load settings from a JSON file\n- Settings: Fixed resolution of settings files paths that are symlinks\n- OTEL: Fixed reporting of wrong organization after authentication changes\n- Slash commands: Fixed permissions checking for allowed-tools with Bash\n- IDE: Added support for pasting images in VSCode MacOS using âŒ˜+V\n- IDE: Added `CLAUDE_CODE_AUTO_CONNECT_IDE=false` for disabling IDE auto-connection\n- Added `CLAUDE_CODE_SHELL_PREFIX` for wrapping Claude and user-provided shell commands run by Claude Code\n\n## 1.0.60\n\n- You can now create custom subagents for specialized tasks! Run /agents to get started\n\n## 1.0.59\n\n- SDK: Added tool confirmation support with canUseTool callback\n- SDK: Allow specifying env for spawned process\n- Hooks: Exposed PermissionDecision to hooks (including \"ask\")\n- Hooks: UserPromptSubmit now supports additionalContext in advanced JSON output\n- Fixed issue where some Max users that specified Opus would still see fallback to Sonnet\n\n## 1.0.58\n\n- Added support for reading PDFs\n- MCP: Improved server health status display in 'claude mcp list'\n- Hooks: Added CLAUDE_PROJECT_DIR env var for hook commands\n\n## 1.0.57\n\n- Added support for specifying a model in slash commands\n- Improved permission messages to help Claude understand allowed tools\n- Fix: Remove trailing newlines from bash output in terminal wrapping\n\n## 1.0.56\n\n- Windows: Enabled shift+tab for mode switching on versions of Node.js that support terminal VT mode\n- Fixes for WSL IDE detection\n- Fix an issue causing awsRefreshHelper changes to .aws directory not to be picked up\n\n## 1.0.55\n\n- Clarified knowledge cutoff for Opus 4 and Sonnet 4 models\n- Windows: fixed Ctrl+Z crash\n- SDK: Added ability to capture error logging\n- Add --system-prompt-file option to override system prompt in print mode\n\n## 1.0.54\n\n- Hooks: Added UserPromptSubmit hook and the current working directory to hook inputs\n- Custom slash commands: Added argument-hint to frontmatter\n- Windows: OAuth uses port 45454 and properly constructs browser URL\n- Windows: mode switching now uses alt + m, and plan mode renders properly\n- Shell: Switch to in-memory shell snapshot to fix file-related errors\n\n## 1.0.53\n\n- Updated @-mention file truncation from 100 lines to 2000 lines\n- Add helper script settings for AWS token refresh: awsAuthRefresh (for foreground operations like aws sso login) and awsCredentialExport (for background operation with STS-like response).\n\n## 1.0.52\n\n- Added support for MCP server instructions\n\n## 1.0.51\n\n- Added support for native Windows (requires Git for Windows)\n- Added support for Bedrock API keys through environment variable AWS_BEARER_TOKEN_BEDROCK\n- Settings: /doctor can now help you identify and fix invalid setting files\n- `--append-system-prompt` can now be used in interactive mode, not just --print/-p.\n- Increased auto-compact warning threshold from 60% to 80%\n- Fixed an issue with handling user directories with spaces for shell snapshots\n- OTEL resource now includes os.type, os.version, host.arch, and wsl.version (if running on Windows Subsystem for Linux)\n- Custom slash commands: Fixed user-level commands in subdirectories\n- Plan mode: Fixed issue where rejected plan from sub-task would get discarded\n\n## 1.0.48\n\n- Fixed a bug in v1.0.45 where the app would sometimes freeze on launch\n- Added progress messages to Bash tool based on the last 5 lines of command output\n- Added expanding variables support for MCP server configuration\n- Moved shell snapshots from /tmp to ~/.claude for more reliable Bash tool calls\n- Improved IDE extension path handling when Claude Code runs in WSL\n- Hooks: Added a PreCompact hook\n- Vim mode: Added c, f/F, t/T\n\n## 1.0.45\n\n- Redesigned Search (Grep) tool with new tool input parameters and features\n- Disabled IDE diffs for notebook files, fixing \"Timeout waiting after 1000ms\" error\n- Fixed config file corruption issue by enforcing atomic writes\n- Updated prompt input undo to Ctrl+\\_ to avoid breaking existing Ctrl+U behavior, matching zsh's undo shortcut\n- Stop Hooks: Fixed transcript path after /clear and fixed triggering when loop ends with tool call\n- Custom slash commands: Restored namespacing in command names based on subdirectories. For example, .claude/commands/frontend/component.md is now /frontend:component, not /component.\n\n## 1.0.44\n\n- New /export command lets you quickly export a conversation for sharing\n- MCP: resource_link tool results are now supported\n- MCP: tool annotations and tool titles now display in /mcp view\n- Changed Ctrl+Z to suspend Claude Code. Resume by running `fg`. Prompt input undo is now Ctrl+U.\n\n## 1.0.43\n\n- Fixed a bug where the theme selector was saving excessively\n- Hooks: Added EPIPE system error handling\n\n## 1.0.42\n\n- Added tilde (`~`) expansion support to `/add-dir` command\n\n## 1.0.41\n\n- Hooks: Split Stop hook triggering into Stop and SubagentStop\n- Hooks: Enabled optional timeout configuration for each command\n- Hooks: Added \"hook_event_name\" to hook input\n- Fixed a bug where MCP tools would display twice in tool list\n- New tool parameters JSON for Bash tool in `tool_decision` event\n\n## 1.0.40\n\n- Fixed a bug causing API connection errors with UNABLE_TO_GET_ISSUER_CERT_LOCALLY if `NODE_EXTRA_CA_CERTS` was set\n\n## 1.0.39\n\n- New Active Time metric in OpenTelemetry logging\n\n## 1.0.38\n\n- Released hooks. Special thanks to community input in https://github.com/anthropics/claude-code/issues/712. Docs: https://docs.anthropic.com/en/docs/claude-code/hooks\n\n## 1.0.37\n\n- Remove ability to set `Proxy-Authorization` header via ANTHROPIC_AUTH_TOKEN or apiKeyHelper\n\n## 1.0.36\n\n- Web search now takes today's date into context\n- Fixed a bug where stdio MCP servers were not terminating properly on exit\n\n## 1.0.35\n\n- Added support for MCP OAuth Authorization Server discovery\n\n## 1.0.34\n\n- Fixed a memory leak causing a MaxListenersExceededWarning message to appear\n\n## 1.0.33\n\n- Improved logging functionality with session ID support\n- Added prompt input undo functionality (Ctrl+Z and vim 'u' command)\n- Improvements to plan mode\n\n## 1.0.32\n\n- Updated loopback config for litellm\n- Added forceLoginMethod setting to bypass login selection screen\n\n## 1.0.31\n\n- Fixed a bug where ~/.claude.json would get reset when file contained invalid JSON\n\n## 1.0.30\n\n- Custom slash commands: Run bash output, @-mention files, enable thinking with thinking keywords\n- Improved file path autocomplete with filename matching\n- Added timestamps in Ctrl-r mode and fixed Ctrl-c handling\n- Enhanced jq regex support for complex filters with pipes and select\n\n## 1.0.29\n\n- Improved CJK character support in cursor navigation and rendering\n\n## 1.0.28\n\n- Slash commands: Fix selector display during history navigation\n- Resizes images before upload to prevent API size limit errors\n- Added XDG_CONFIG_HOME support to configuration directory\n- Performance optimizations for memory usage\n- New attributes (terminal.type, language) in OpenTelemetry logging\n\n## 1.0.27\n\n- Streamable HTTP MCP servers are now supported\n- Remote MCP servers (SSE and HTTP) now support OAuth\n- MCP resources can now be @-mentioned\n- /resume slash command to switch conversations within Claude Code\n\n## 1.0.25\n\n- Slash commands: moved \"project\" and \"user\" prefixes to descriptions\n- Slash commands: improved reliability for command discovery\n- Improved support for Ghostty\n- Improved web search reliability\n\n## 1.0.24\n\n- Improved /mcp output\n- Fixed a bug where settings arrays got overwritten instead of merged\n\n## 1.0.23\n\n- Released TypeScript SDK: import @anthropic-ai/claude-code to get started\n- Released Python SDK: pip install claude-code-sdk to get started\n\n## 1.0.22\n\n- SDK: Renamed `total_cost` to `total_cost_usd`\n\n## 1.0.21\n\n- Improved editing of files with tab-based indentation\n- Fix for tool_use without matching tool_result errors\n- Fixed a bug where stdio MCP server processes would linger after quitting Claude Code\n\n## 1.0.18\n\n- Added --add-dir CLI argument for specifying additional working directories\n- Added streaming input support without require -p flag\n- Improved startup performance and session storage performance\n- Added CLAUDE_BASH_MAINTAIN_PROJECT_WORKING_DIR environment variable to freeze working directory for bash commands\n- Added detailed MCP server tools display (/mcp)\n- MCP authentication and permission improvements\n- Added auto-reconnection for MCP SSE connections on disconnect\n- Fixed issue where pasted content was lost when dialogs appeared\n\n## 1.0.17\n\n- We now emit messages from sub-tasks in -p mode (look for the parent_tool_use_id property)\n- Fixed crashes when the VS Code diff tool is invoked multiple times quickly\n- MCP server list UI improvements\n- Update Claude Code process title to display \"claude\" instead of \"node\"\n\n## 1.0.11\n\n- Claude Code can now also be used with a Claude Pro subscription\n- Added /upgrade for smoother switching to Claude Max plans\n- Improved UI for authentication from API keys and Bedrock/Vertex/external auth tokens\n- Improved shell configuration error handling\n- Improved todo list handling during compaction\n\n## 1.0.10\n\n- Added markdown table support\n- Improved streaming performance\n\n## 1.0.8\n\n- Fixed Vertex AI region fallback when using CLOUD_ML_REGION\n- Increased default otel interval from 1s -> 5s\n- Fixed edge cases where MCP_TIMEOUT and MCP_TOOL_TIMEOUT weren't being respected\n- Fixed a regression where search tools unnecessarily asked for permissions\n- Added support for triggering thinking non-English languages\n- Improved compacting UI\n\n## 1.0.7\n\n- Renamed /allowed-tools -> /permissions\n- Migrated allowedTools and ignorePatterns from .claude.json -> settings.json\n- Deprecated claude config commands in favor of editing settings.json\n- Fixed a bug where --dangerously-skip-permissions sometimes didn't work in --print mode\n- Improved error handling for /install-github-app\n- Bugfixes, UI polish, and tool reliability improvements\n\n## 1.0.6\n\n- Improved edit reliability for tab-indented files\n- Respect CLAUDE_CONFIG_DIR everywhere\n- Reduced unnecessary tool permission prompts\n- Added support for symlinks in @file typeahead\n- Bugfixes, UI polish, and tool reliability improvements\n\n## 1.0.4\n\n- Fixed a bug where MCP tool errors weren't being parsed correctly\n\n## 1.0.1\n\n- Added `DISABLE_INTERLEAVED_THINKING` to give users the option to opt out of interleaved thinking.\n- Improved model references to show provider-specific names (Sonnet 3.7 for Bedrock, Sonnet 4 for Console)\n- Updated documentation links and OAuth process descriptions\n\n## 1.0.0\n\n- Claude Code is now generally available\n- Introducing Sonnet 4 and Opus 4 models\n\n## 0.2.125\n\n- Breaking change: Bedrock ARN passed to `ANTHROPIC_MODEL` or `ANTHROPIC_SMALL_FAST_MODEL` should no longer contain an escaped slash (specify `/` instead of `%2F`)\n- Removed `DEBUG=true` in favor of `ANTHROPIC_LOG=debug`, to log all requests\n\n## 0.2.117\n\n- Breaking change: --print JSON output now returns nested message objects, for forwards-compatibility as we introduce new metadata fields\n- Introduced settings.cleanupPeriodDays\n- Introduced CLAUDE_CODE_API_KEY_HELPER_TTL_MS env var\n- Introduced --debug mode\n\n## 0.2.108\n\n- You can now send messages to Claude while it works to steer Claude in real-time\n- Introduced BASH_DEFAULT_TIMEOUT_MS and BASH_MAX_TIMEOUT_MS env vars\n- Fixed a bug where thinking was not working in -p mode\n- Fixed a regression in /cost reporting\n- Deprecated MCP wizard interface in favor of other MCP commands\n- Lots of other bugfixes and improvements\n\n## 0.2.107\n\n- CLAUDE.md files can now import other files. Add @path/to/file.md to ./CLAUDE.md to load additional files on launch\n\n## 0.2.106\n\n- MCP SSE server configs can now specify custom headers\n- Fixed a bug where MCP permission prompt didn't always show correctly\n\n## 0.2.105\n\n- Claude can now search the web\n- Moved system & account status to /status\n- Added word movement keybindings for Vim\n- Improved latency for startup, todo tool, and file edits\n\n## 0.2.102\n\n- Improved thinking triggering reliability\n- Improved @mention reliability for images and folders\n- You can now paste multiple large chunks into one prompt\n\n## 0.2.100\n\n- Fixed a crash caused by a stack overflow error\n- Made db storage optional; missing db support disables --continue and --resume\n\n## 0.2.98\n\n- Fixed an issue where auto-compact was running twice\n\n## 0.2.96\n\n- Claude Code can now also be used with a Claude Max subscription (https://claude.ai/upgrade)\n\n## 0.2.93\n\n- Resume conversations from where you left off from with \"claude --continue\" and \"claude --resume\"\n- Claude now has access to a Todo list that helps it stay on track and be more organized\n\n## 0.2.82\n\n- Added support for --disallowedTools\n- Renamed tools for consistency: LSTool -> LS, View -> Read, etc.\n\n## 0.2.75\n\n- Hit Enter to queue up additional messages while Claude is working\n- Drag in or copy/paste image files directly into the prompt\n- @-mention files to directly add them to context\n- Run one-off MCP servers with `claude --mcp-config <path-to-file>`\n- Improved performance for filename auto-complete\n\n## 0.2.74\n\n- Added support for refreshing dynamically generated API keys (via apiKeyHelper), with a 5 minute TTL\n- Task tool can now perform writes and run bash commands\n\n## 0.2.72\n\n- Updated spinner to indicate tokens loaded and tool usage\n\n## 0.2.70\n\n- Network commands like curl are now available for Claude to use\n- Claude can now run multiple web queries in parallel\n- Pressing ESC once immediately interrupts Claude in Auto-accept mode\n\n## 0.2.69\n\n- Fixed UI glitches with improved Select component behavior\n- Enhanced terminal output display with better text truncation logic\n\n## 0.2.67\n\n- Shared project permission rules can be saved in .claude/settings.json\n\n## 0.2.66\n\n- Print mode (-p) now supports streaming output via --output-format=stream-json\n- Fixed issue where pasting could trigger memory or bash mode unexpectedly\n\n## 0.2.63\n\n- Fixed an issue where MCP tools were loaded twice, which caused tool call errors\n\n## 0.2.61\n\n- Navigate menus with vim-style keys (j/k) or bash/emacs shortcuts (Ctrl+n/p) for faster interaction\n- Enhanced image detection for more reliable clipboard paste functionality\n- Fixed an issue where ESC key could crash the conversation history selector\n\n## 0.2.59\n\n- Copy+paste images directly into your prompt\n- Improved progress indicators for bash and fetch tools\n- Bugfixes for non-interactive mode (-p)\n\n## 0.2.54\n\n- Quickly add to Memory by starting your message with '#'\n- Press ctrl+r to see full output for long tool results\n- Added support for MCP SSE transport\n\n## 0.2.53\n\n- New web fetch tool lets Claude view URLs that you paste in\n- Fixed a bug with JPEG detection\n\n## 0.2.50\n\n- New MCP \"project\" scope now allows you to add MCP servers to .mcp.json files and commit them to your repository\n\n## 0.2.49\n\n- Previous MCP server scopes have been renamed: previous \"project\" scope is now \"local\" and \"global\" scope is now \"user\"\n\n## 0.2.47\n\n- Press Tab to auto-complete file and folder names\n- Press Shift + Tab to toggle auto-accept for file edits\n- Automatic conversation compaction for infinite conversation length (toggle with /config)\n\n## 0.2.44\n\n- Ask Claude to make a plan with thinking mode: just say 'think' or 'think harder' or even 'ultrathink'\n\n## 0.2.41\n\n- MCP server startup timeout can now be configured via MCP_TIMEOUT environment variable\n- MCP server startup no longer blocks the app from starting up\n\n## 0.2.37\n\n- New /release-notes command lets you view release notes at any time\n- `claude config add/remove` commands now accept multiple values separated by commas or spaces\n\n## 0.2.36\n\n- Import MCP servers from Claude Desktop with `claude mcp add-from-claude-desktop`\n- Add MCP servers as JSON strings with `claude mcp add-json <n> <json>`\n\n## 0.2.34\n\n- Vim bindings for text input - enable with /vim or /config\n\n## 0.2.32\n\n- Interactive MCP setup wizard: Run \"claude mcp add\" to add MCP servers with a step-by-step interface\n- Fix for some PersistentShell issues\n\n## 0.2.31\n\n- Custom slash commands: Markdown files in .claude/commands/ directories now appear as custom slash commands to insert prompts into your conversation\n- MCP debug mode: Run with --mcp-debug flag to get more information about MCP server errors\n\n## 0.2.30\n\n- Added ANSI color theme for better terminal compatibility\n- Fixed issue where slash command arguments weren't being sent properly\n- (Mac-only) API keys are now stored in macOS Keychain\n\n## 0.2.26\n\n- New /approved-tools command for managing tool permissions\n- Word-level diff display for improved code readability\n- Fuzzy matching for slash commands\n\n## 0.2.21\n\n- Fuzzy matching for /commands\n",
  "changelogLastFetched": 1754261663285,
  "lastReleaseNotesSeen": "1.0.67",
  "subscriptionNoticeCount": 0,
  "hasAvailableSubscription": false,
  "fallbackAvailableWarningThreshold": 0.2
}